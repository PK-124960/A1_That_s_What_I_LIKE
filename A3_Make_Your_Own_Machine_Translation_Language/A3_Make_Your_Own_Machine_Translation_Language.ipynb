{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1bf85fb7-061e-4550-87a6-6d2ca19bb26e",
   "metadata": {},
   "source": [
    "## Dataset Preparation for Thai-English Neural Machine Translation (NMT) \n",
    "\n",
    "The SCB-MT English-Thai Dataset is used for training a translation model. Below is a detailed breakdown of how the dataset is prepared, including text normalization, tokenization, and word segmentation, particularly handling the Thai language's unique linguistic structure.\n",
    "\n",
    "- SCB-MT English-Thai Dataset\n",
    "\n",
    "    ðŸ“Œ Credit: https://github.com/vistec-AI/dataset-releases/releases/tag/scb-mt-en-th-2020_v1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d472f02-8d71-4b5f-b67f-1b550b179ca0",
   "metadata": {},
   "source": [
    "# TASK 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1cc833-d9d0-4ca0-a8d7-8d65dc8f4609",
   "metadata": {},
   "source": [
    "### Install Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d619bf7-afe5-4f7c-b436-4749a4b6e761",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install numpy==1.26.4 --no-cache-dir\n",
    "# !pip install torch==2.1.2\n",
    "# !pip install torchtext==0.16.2  # Adjust based on your Torch version\n",
    "# !pip install pandas \n",
    "# !pip install pythainlp \n",
    "# !pip install spacy\n",
    "# !pip install spacy_thai\n",
    "# !pip install matplotlib\n",
    "# !pip install scikit-learn\n",
    "# !python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68db2963-05ca-4296-9a96-12196e6c09ee",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44668455-46ce-4545-a208-7536064ebd43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "import spacy\n",
    "from pythainlp.tokenize import word_tokenize\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8cf0c59a-9847-48ff-8005-c8e38990df01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# âœ… Set Device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cca96ec-3139-4351-b8b6-b6db3f8bf91f",
   "metadata": {},
   "source": [
    "### Define Text Normalization\n",
    "\n",
    "to clean and standardize text before tokenization by converting all text to lowercase, removing punctuation, special characters, and extra whitespace, including handling missing/null values by skipping them.\n",
    "  \n",
    "ðŸ“Œ Tools Used: \n",
    "- re (Regular Expressions): Cleans unwanted characters efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1061f049-dad9-4251-a7c0-e7d0272fe298",
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… Normalize Text (Remove special characters and lower case)\n",
    "def normalize_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
    "    return text.lower().strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fde9655-0f47-4407-acc6-68852dd7dfca",
   "metadata": {},
   "source": [
    "### Tokenization (English & Thai)\n",
    "\n",
    "ðŸ“Œ English Tokenization\n",
    "- spaCy (en_core_web_sm): Provides high-quality word-level tokenization.\n",
    "  \n",
    "ðŸ“Œ Thai Tokenization (word segmentation): \n",
    "- PyThaiNLP (word_tokenize): Accurately segments Thai text into words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a6ec7f5-99f1-4fd0-be80-dcb885243f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… Load Tokenizers\n",
    "nlp_en = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def tokenize_en(text):\n",
    "    return [token.text for token in nlp_en(text.lower().strip())]\n",
    "\n",
    "def tokenize_th(text):\n",
    "    return word_tokenize(text.lower().strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50adb115-2a59-41ec-b1b3-9f56bccfe387",
   "metadata": {},
   "source": [
    "### Load Dataset (SCB-MT English-Thai)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93c5457-1691-441e-ab6f-23c074204e0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 154036 sentence pairs.\n",
      "train_data: 138632, valid_data: 15404\n"
     ]
    }
   ],
   "source": [
    "# âœ… Load dataset\n",
    "def load_dataset(folder_path):\n",
    "    dataset = []\n",
    "    for file in os.listdir(folder_path):\n",
    "        if file.endswith(\".csv\"):\n",
    "            df = pd.read_csv(os.path.join(folder_path, file)).dropna(subset=[\"en_text\", \"th_text\"])\n",
    "            dataset.extend(zip(df[\"en_text\"], df[\"th_text\"]))\n",
    "    return dataset\n",
    "\n",
    "dataset_folder = \"scb-mt-en-th-2020_forTest\"\n",
    "dataset = load_dataset(dataset_folder)\n",
    "print(f\"Loaded {len(dataset)} sentence pairs.\")\n",
    "\n",
    "# âœ… Split dataset into training and validation\n",
    "train_data, valid_data = train_test_split(dataset, test_size=0.1, random_state=42)\n",
    "print(f\"train_data: {len(train_data)}, valid_data: {len(valid_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a40bbfa5-462b-46d9-b8d3-23ff4b277da3",
   "metadata": {},
   "source": [
    "### Vocabulary Construction\n",
    "\n",
    "- To train a neural network, words must be converted into numerical representations.\n",
    "- A vocabulary (word-to-index mapping) is built from the dataset.\n",
    "- Special tokens are added:\n",
    "    - <unk> (Unknown word)\n",
    "    - <pad> (Padding token)\n",
    "    - <sos> (Start of sentence)\n",
    "    - <eos> (End of sentence)\n",
    "\n",
    "ðŸ“Œ Tools Used:\n",
    "- torchtext (build_vocab_from_iterator): Creates vocabularies from tokenized text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89b7f038-2aec-4d3d-a7ea-62110a3b502a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… Build Vocabularies\n",
    "special_symbols = ['<unk>', '<pad>', '<sos>', '<eos>']\n",
    "\n",
    "def yield_tokens(data, language):\n",
    "    for src, trg in data:\n",
    "        yield tokenize_en(src) if language == \"en\" else tokenize_th(trg)\n",
    "\n",
    "vocab_en = build_vocab_from_iterator(yield_tokens(dataset, \"en\"), min_freq=2, specials=special_symbols, special_first=True)\n",
    "vocab_th = build_vocab_from_iterator(yield_tokens(dataset, \"th\"), min_freq=2, specials=special_symbols, special_first=True)\n",
    "\n",
    "vocab_en.set_default_index(vocab_en['<unk>'])\n",
    "vocab_th.set_default_index(vocab_th['<unk>'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36616484-5d12-4a82-9b58-221b4328cb66",
   "metadata": {},
   "source": [
    "### DataLoader for Model Training\n",
    "\n",
    "- Converts tokenized sentences into numerical tensor representations.\n",
    "- Uses PyTorch's DataLoader for efficient batching.\n",
    "\n",
    "ðŸ“Œ Tools Used:\n",
    "- PyTorch (torch.utils.data.DataLoader): Handles batching & padding efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "86f0d4da-ca67-4f0d-8f8a-f4cd6a9a8d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… Text to tensor transformation\n",
    "def tensor_transform(tokens, vocab):\n",
    "    return torch.tensor([vocab['<sos>']] + [vocab[token] for token in tokens] + [vocab['<eos>']], dtype=torch.long)\n",
    "\n",
    "def text_pipeline_en(text):\n",
    "    return tensor_transform(tokenize_en(text), vocab_en)\n",
    "\n",
    "def text_pipeline_th(text):\n",
    "    return tensor_transform(tokenize_th(text), vocab_th)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0287778f-c143-4a4f-819f-f509174f70a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… DataLoader (Corrected and moved before training loop)\n",
    "def collate_batch(batch):\n",
    "    src_batch, src_len_batch, trg_batch = [], [], []\n",
    "    for src, trg in batch:\n",
    "        src_tokens = text_pipeline_en(src)\n",
    "        trg_tokens = text_pipeline_th(trg)\n",
    "        src_batch.append(src_tokens)\n",
    "        trg_batch.append(trg_tokens)\n",
    "        src_len_batch.append(len(src_tokens))\n",
    "\n",
    "    src_batch = pad_sequence(src_batch, padding_value=vocab_en['<pad>'], batch_first=True)\n",
    "    trg_batch = pad_sequence(trg_batch, padding_value=vocab_th['<pad>'], batch_first=True)\n",
    "\n",
    "    return src_batch.to(device), torch.tensor(src_len_batch, dtype=torch.int64).to(device), trg_batch.to(device)\n",
    "\n",
    "batch_size = 64  # Or your desired batch size\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_batch)\n",
    "\n",
    "# âœ… Performance Tracking\n",
    "performance_metrics = []\n",
    "\n",
    "def safe_exp(value):\n",
    "    return torch.exp(torch.tensor(value)).item() if value < 10 else float('inf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba1b3dd-6183-42d5-9ae1-9780c0511ab9",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "- Thai requires special handling due to the lack of spaces in writing. PyThaiNLP is used for word segmentation.\n",
    "- English uses spaCy, which provides reliable tokenization.\n",
    "- The dataset is loaded and cleaned to ensure it is suitable for training.\n",
    "- The final output is numerical tensors, ready to be used in a Neural Machine Translation (NMT) model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b95fe5c-fd97-4cc9-bc42-b15dad8d7977",
   "metadata": {},
   "source": [
    "## README.md\n",
    "\n",
    "- The dataset source: SCB-MT English-Thai Dataset\n",
    "\n",
    "ðŸ“Œ Credit: https://github.com/vistec-AI/dataset-releases/releases/tag/scb-mt-en-th-2020_v1.0\n",
    "\n",
    "- We use PyThaiNLP for Thai tokenization and spaCy for English tokenization.\n",
    "- Thai tokenization requires word segmentation since it does not use spaces."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af751332-ca42-4b12-a871-74342bcf5295",
   "metadata": {},
   "source": [
    "# TASK 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a01131be-d841-4daa-a5fe-76b703a12d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… Encoder\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, hid_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        self.rnn = nn.LSTM(emb_dim, hid_dim, bidirectional=True, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear = nn.Linear(2 * hid_dim, hid_dim)  # Linear layer for encoder outputs\n",
    "\n",
    "    def forward(self, src, src_len):\n",
    "        embedded = self.dropout(self.embedding(src))\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, src_len.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        packed_outputs, (hidden, cell) = self.rnn(packed_embedded)\n",
    "        outputs, _ = nn.utils.rnn.pad_packed_sequence(packed_outputs, batch_first=True)\n",
    "\n",
    "        hidden = (hidden[0] + hidden[1]) / 2\n",
    "        cell = (cell[0] + cell[1]) / 2\n",
    "\n",
    "        outputs = self.linear(outputs)  # Apply linear transformation to encoder outputs\n",
    "\n",
    "        return outputs, hidden, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e77a21b7-8a46-436b-8b43-6b9ed9e00001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… Decoder\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout, attention):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        self.attention = attention\n",
    "        self.rnn = nn.LSTM(emb_dim + enc_hid_dim, dec_hid_dim, batch_first=True)\n",
    "        self.fc_out = nn.Linear(enc_hid_dim + dec_hid_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, input, hidden, cell, encoder_outputs):\n",
    "        input = input.unsqueeze(1)\n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "\n",
    "        attn_weights = self.attention(hidden, encoder_outputs).unsqueeze(1)\n",
    "        context = torch.bmm(attn_weights, encoder_outputs)\n",
    "\n",
    "        rnn_input = torch.cat((embedded, context), dim=2)\n",
    "        output, (hidden, cell) = self.rnn(rnn_input, (hidden.unsqueeze(0), cell.unsqueeze(0)))\n",
    "\n",
    "        prediction = self.fc_out(torch.cat((output.squeeze(1), context.squeeze(1)), dim=1))\n",
    "        return prediction, hidden.squeeze(0), cell.squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "07c06e59-023c-4f24-8d2d-1b70b4f3c22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… Seq2Seq Model\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, src, src_len, trg):\n",
    "        encoder_outputs, hidden, cell = self.encoder(src, src_len)\n",
    "        input_ = trg[:, 0]\n",
    "        outputs = []\n",
    "        for t in range(1, trg.shape[1]):\n",
    "            output, hidden, cell = self.decoder(input_, hidden, cell, encoder_outputs)\n",
    "            outputs.append(output)\n",
    "            input_ = output.argmax(1)\n",
    "        return torch.stack(outputs, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6a427ca7-bd8d-488b-8a93-10936391eeee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… General Attention (Dot Product)\n",
    "class GeneralAttention(nn.Module):\n",
    "    def __init__(self, enc_hid_dim, dec_hid_dim):  # Add __init__\n",
    "        super().__init__()\n",
    "        # You can add a linear layer here if needed for dimension matching in the future\n",
    "        # self.W = nn.Linear(dec_hid_dim, enc_hid_dim)  # Example if dimensions need adjustment\n",
    "\n",
    "    def forward(self, s, h):\n",
    "        # If you added a linear layer in __init__, use it here:\n",
    "        # s = self.W(s)\n",
    "        attn_scores = torch.bmm(h, s.unsqueeze(2)).squeeze(2)\n",
    "        return torch.softmax(attn_scores, dim=1)\n",
    "\n",
    "# âœ… Multiplicative Attention (Dot Product with Weight)\n",
    "class MultiplicativeAttention(nn.Module):\n",
    "    def __init__(self, enc_hid_dim, dec_hid_dim):  # Add __init__\n",
    "        super().__init__()\n",
    "        self.W = nn.Linear(dec_hid_dim, enc_hid_dim, bias=False)  # Corrected: dec_hid_dim -> enc_hid_dim\n",
    "\n",
    "    def forward(self, s, h):\n",
    "        s = self.W(s).unsqueeze(2)\n",
    "        attn_scores = torch.bmm(h, s).squeeze(2)\n",
    "        return torch.softmax(attn_scores, dim=1)\n",
    "\n",
    "# âœ… Additive Attention (Bahdanau)\n",
    "class AdditiveAttention(nn.Module):\n",
    "    def __init__(self, enc_hid_dim, dec_hid_dim):  # Add __init__\n",
    "        super().__init__()\n",
    "        self.W1 = nn.Linear(enc_hid_dim, dec_hid_dim)\n",
    "        self.W2 = nn.Linear(dec_hid_dim, dec_hid_dim)\n",
    "        self.v = nn.Linear(dec_hid_dim, 1, bias=False)\n",
    "\n",
    "    def forward(self, s, h):\n",
    "        s_expanded = self.W2(s).unsqueeze(1)\n",
    "        energy = torch.tanh(self.W1(h) + s_expanded)\n",
    "        attn_scores = self.v(energy).squeeze(2)\n",
    "        return torch.softmax(attn_scores, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9fbc018-f1d4-41d5-8712-6cc89b6bfd51",
   "metadata": {},
   "source": [
    "## README.md\n",
    "\n",
    "- General Attention:\n",
    "e_i = s^T h_i\n",
    "\n",
    "- Multiplicative Attention:\n",
    "e_i = s^T W h_i, where W âˆˆ R^(d2Ã—d1)\n",
    "\n",
    "- Additive Attention:\n",
    "e_i = v^T tanh(W1 h_i + W2 s)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b9970c-846f-43eb-b167-93c46871b1de",
   "metadata": {},
   "source": [
    "## Task 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5b138de7-77b0-4bcf-8a04-a32ec82944a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… Task 3: Training and Evaluation for All Attention Types\n",
    "attention_types = {\"general\": GeneralAttention, \"multiplicative\": MultiplicativeAttention, \"additive\": AdditiveAttention}\n",
    "results = {}\n",
    "\n",
    "def plot_attention(attn_weights, input_tokens, output_tokens):\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    cax = ax.matshow(attn_weights.cpu().detach().numpy(), cmap='viridis')\n",
    "    fig.colorbar(cax)\n",
    "    ax.set_xticklabels([''] + input_tokens, rotation=90)\n",
    "    ax.set_yticklabels([''] + output_tokens)\n",
    "    plt.xlabel(\"Input Sequence\")\n",
    "    plt.ylabel(\"Output Sequence\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2e7fd664-bfeb-450d-bad9-902265a7847a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training with general attention...\n",
      "Epoch 1: Train Loss: 6.6681, Train PPL: 786.9373, Validation Loss: 6.4250, Validation PPL: 617.0862\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 2.23 GiB. GPU 0 has a total capacity of 10.75 GiB of which 1.07 GiB is free. Process 3479620 has 1.75 GiB memory in use. Including non-PyTorch memory, this process has 7.93 GiB memory in use. Of the allocated memory 3.59 GiB is allocated by PyTorch, and 4.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 37\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m src, src_len, trg \u001b[38;5;129;01min\u001b[39;00m dataloader:\n\u001b[0;32m---> 37\u001b[0m         output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m         trg \u001b[38;5;241m=\u001b[39m trg[:, \u001b[38;5;241m1\u001b[39m:]\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     39\u001b[0m         output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, output\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[12], line 16\u001b[0m, in \u001b[0;36mSeq2Seq.forward\u001b[0;34m(self, src, src_len, trg)\u001b[0m\n\u001b[1;32m     14\u001b[0m     outputs\u001b[38;5;241m.\u001b[39mappend(output)\n\u001b[1;32m     15\u001b[0m     input_ \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39margmax(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 16\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 2.23 GiB. GPU 0 has a total capacity of 10.75 GiB of which 1.07 GiB is free. Process 3479620 has 1.75 GiB memory in use. Including non-PyTorch memory, this process has 7.93 GiB memory in use. Of the allocated memory 3.59 GiB is allocated by PyTorch, and 4.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "for attn_name, attn_class in attention_types.items():\n",
    "    print(f\"\\nTraining with {attn_name} attention...\")\n",
    "    \n",
    "    encoder = Encoder(len(vocab_en), 256, 512, 0.5).to(device)\n",
    "    attention = attn_class(512, 512).to(device)\n",
    "    decoder = Decoder(len(vocab_th), 256, 512, 512, 0.5, attention).to(device)\n",
    "    model = Seq2Seq(encoder, decoder).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.0005)\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=vocab_th['<pad>'])\n",
    "\n",
    "    training_losses, training_ppls, validation_losses, validation_ppls = [], [], [], []\n",
    "    start_time = time.time()\n",
    "\n",
    "    num_epochs = 5\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for src, src_len, trg in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            output = model(src, src_len, trg)\n",
    "            trg = trg[:, 1:].reshape(-1)\n",
    "            output = output.reshape(-1, output.shape[-1])\n",
    "            mask = trg != vocab_th['<pad>']\n",
    "            output, trg = output[mask], trg[mask]\n",
    "            loss = criterion(output, trg)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        train_loss /= len(dataloader)\n",
    "        training_losses.append(train_loss)\n",
    "        training_ppls.append(safe_exp(train_loss))\n",
    "        \n",
    "        model.eval()\n",
    "        valid_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for src, src_len, trg in dataloader:\n",
    "                output = model(src, src_len, trg)\n",
    "                trg = trg[:, 1:].reshape(-1)\n",
    "                output = output.reshape(-1, output.shape[-1])\n",
    "                mask = trg != vocab_th['<pad>']\n",
    "                output, trg = output[mask], trg[mask]\n",
    "                loss = criterion(output, trg)\n",
    "                valid_loss += loss.item()\n",
    "        valid_loss /= len(dataloader)\n",
    "        validation_losses.append(valid_loss)\n",
    "        validation_ppls.append(safe_exp(valid_loss))\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}: Train Loss: {train_loss:.4f}, Train PPL: {training_ppls[-1]:.4f}, Validation Loss: {valid_loss:.4f}, Validation PPL: {validation_ppls[-1]:.4f}\")\n",
    "    \n",
    "    end_time = time.time()\n",
    "    training_time = end_time - start_time\n",
    "    performance_metrics.append([attn_name, training_time])\n",
    "    \n",
    "    results[attn_name] = {\n",
    "        \"Training Loss\": training_losses,\n",
    "        \"Training PPL\": training_ppls,\n",
    "        \"Validation Loss\": validation_losses,\n",
    "        \"Validation PPL\": validation_ppls,\n",
    "    }\n",
    "\n",
    "def plot_losses(results):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    for attn_type, metrics in results.items():\n",
    "        plt.plot(metrics[\"Training Loss\"], label=f\"{attn_type} - Train Loss\")\n",
    "        plt.plot(metrics[\"Validation Loss\"], linestyle=\"dashed\", label=f\"{attn_type} - Validation Loss\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Training vs Validation Loss\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "plot_losses(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc5f5c9-3bd9-4c5b-bb62-a3f581e48a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… Print Performance Metrics\n",
    "print(\"\\nPerformance Metrics (Training Time per Attention Type):\")\n",
    "for attn_type, time_taken in performance_metrics:\n",
    "    print(f\"{attn_type}: {time_taken:.2f} seconds\")\n",
    "\n",
    "print(\"âœ… Training Completed for All Attention Types\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beea70f7-d8ca-4008-8627-06e9cef2b900",
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… Save Trained Model Components\n",
    "torch.save(encoder, \"encoder.pth\")\n",
    "torch.save(decoder, \"decoder.pth\")\n",
    "torch.save(vocab_en, \"vocab_en.pth\")\n",
    "torch.save(vocab_th, \"vocab_th.pth\")\n",
    "\n",
    "print(\"âœ… Model saved successfully!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "a3.venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
