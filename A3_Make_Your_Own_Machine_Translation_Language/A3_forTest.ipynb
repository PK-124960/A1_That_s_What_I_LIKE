{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 50\u001b[0m\n\u001b[0;32m     47\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m src, trg \u001b[38;5;129;01min\u001b[39;00m data:\n\u001b[0;32m     48\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m tokenize_en(src) \u001b[38;5;28;01mif\u001b[39;00m language \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124men\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m tokenize_th(trg)\n\u001b[1;32m---> 50\u001b[0m vocab_en \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_vocab_from_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43myield_tokens\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43men\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspecials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mspecial_symbols\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspecial_first\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     51\u001b[0m vocab_th \u001b[38;5;241m=\u001b[39m build_vocab_from_iterator(yield_tokens(train_data, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mth\u001b[39m\u001b[38;5;124m\"\u001b[39m), min_freq\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, specials\u001b[38;5;241m=\u001b[39mspecial_symbols, special_first\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     53\u001b[0m vocab_en\u001b[38;5;241m.\u001b[39mset_default_index(vocab_en[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<unk>\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[1;32mg:\\My Drive\\AT82.05 NLU\\Assignments\\NLP Assignments\\NPL_Assignments\\A3_Make_Your_Own_Machine_Translation_Language\\a3.venv\\lib\\site-packages\\torchtext\\vocab\\vocab_factory.py:98\u001b[0m, in \u001b[0;36mbuild_vocab_from_iterator\u001b[1;34m(iterator, min_freq, specials, special_first, max_tokens)\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;124;03mBuild a Vocab from an iterator.\u001b[39;00m\n\u001b[0;32m     74\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;124;03m    >>> vocab = build_vocab_from_iterator(yield_tokens(file_path), specials=[\"<unk>\"])\u001b[39;00m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     97\u001b[0m counter \u001b[38;5;241m=\u001b[39m Counter()\n\u001b[1;32m---> 98\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m tokens \u001b[38;5;129;01min\u001b[39;00m iterator:\n\u001b[0;32m     99\u001b[0m     counter\u001b[38;5;241m.\u001b[39mupdate(tokens)\n\u001b[0;32m    101\u001b[0m specials \u001b[38;5;241m=\u001b[39m specials \u001b[38;5;129;01mor\u001b[39;00m []\n",
      "Cell \u001b[1;32mIn[1], line 48\u001b[0m, in \u001b[0;36myield_tokens\u001b[1;34m(data, language)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21myield_tokens\u001b[39m(data, language):\n\u001b[0;32m     47\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m src, trg \u001b[38;5;129;01min\u001b[39;00m data:\n\u001b[1;32m---> 48\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[43mtokenize_en\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m language \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124men\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m tokenize_th(trg)\n",
      "Cell \u001b[1;32mIn[1], line 23\u001b[0m, in \u001b[0;36mtokenize_en\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mtokenize_en\u001b[39m(text):\n\u001b[1;32m---> 23\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [token\u001b[38;5;241m.\u001b[39mtext \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m \u001b[43mnlp_en\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlower\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstrip\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m]\n",
      "File \u001b[1;32mg:\\My Drive\\AT82.05 NLU\\Assignments\\NLP Assignments\\NPL_Assignments\\A3_Make_Your_Own_Machine_Translation_Language\\a3.venv\\lib\\site-packages\\spacy\\language.py:1052\u001b[0m, in \u001b[0;36mLanguage.__call__\u001b[1;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[0;32m   1050\u001b[0m     error_handler \u001b[38;5;241m=\u001b[39m proc\u001b[38;5;241m.\u001b[39mget_error_handler()\n\u001b[0;32m   1051\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1052\u001b[0m     doc \u001b[38;5;241m=\u001b[39m proc(doc, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcomponent_cfg\u001b[38;5;241m.\u001b[39mget(name, {}))  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m   1053\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1054\u001b[0m     \u001b[38;5;66;03m# This typically happens if a component is not initialized\u001b[39;00m\n\u001b[0;32m   1055\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE109\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname)) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "import spacy\n",
    "from pythainlp.tokenize import word_tokenize\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "\n",
    "# ✅ Task 1: Data Preprocessing\n",
    "# ✅ Device setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ✅ Tokenization\n",
    "nlp_en = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def tokenize_en(text):\n",
    "    return [token.text for token in nlp_en(text.lower().strip())]\n",
    "\n",
    "def tokenize_th(text):\n",
    "    return word_tokenize(text.lower().strip())\n",
    "\n",
    "# ✅ Load dataset\n",
    "def load_dataset(folder_path):\n",
    "    dataset = []\n",
    "    for file in os.listdir(folder_path):\n",
    "        if file.endswith(\".csv\"):\n",
    "            df = pd.read_csv(os.path.join(folder_path, file)).dropna(subset=[\"en_text\", \"th_text\"])\n",
    "            dataset.extend(zip(df[\"en_text\"], df[\"th_text\"]))\n",
    "    return dataset\n",
    "\n",
    "dataset_folder = \"scb-mt-en-th-2020_forTest\"\n",
    "dataset = load_dataset(dataset_folder)\n",
    "\n",
    "# ✅ Split dataset into training and validation\n",
    "train_data, valid_data = train_test_split(dataset, test_size=0.1, random_state=42)\n",
    "\n",
    "# ✅ Build vocabulary\n",
    "special_symbols = ['<unk>', '<pad>', '<sos>', '<eos>']\n",
    "\n",
    "def yield_tokens(data, language):\n",
    "    for src, trg in data:\n",
    "        yield tokenize_en(src) if language == \"en\" else tokenize_th(trg)\n",
    "\n",
    "vocab_en = build_vocab_from_iterator(yield_tokens(train_data, \"en\"), min_freq=2, specials=special_symbols, special_first=True)\n",
    "vocab_th = build_vocab_from_iterator(yield_tokens(train_data, \"th\"), min_freq=2, specials=special_symbols, special_first=True)\n",
    "\n",
    "vocab_en.set_default_index(vocab_en['<unk>'])\n",
    "vocab_th.set_default_index(vocab_th['<unk>'])\n",
    "\n",
    "# ✅ Text to tensor transformation (Moved before DataLoader)\n",
    "def tensor_transform(tokens, vocab):\n",
    "    return torch.tensor([vocab['<sos>']] + [vocab[token] for token in tokens] + [vocab['<eos>']], dtype=torch.long)\n",
    "\n",
    "def text_pipeline_en(text):\n",
    "    return tensor_transform(tokenize_en(text), vocab_en)\n",
    "\n",
    "def text_pipeline_th(text):\n",
    "    return tensor_transform(tokenize_th(text), vocab_th)\n",
    "\n",
    "# ✅ DataLoader (Corrected and moved before training loop)\n",
    "def collate_batch(batch):\n",
    "    src_batch, src_len_batch, trg_batch = [], [], []\n",
    "    for src, trg in batch:\n",
    "        src_tokens = text_pipeline_en(src)\n",
    "        trg_tokens = text_pipeline_th(trg)\n",
    "        src_batch.append(src_tokens)\n",
    "        trg_batch.append(trg_tokens)\n",
    "        src_len_batch.append(len(src_tokens))\n",
    "\n",
    "    src_batch = pad_sequence(src_batch, padding_value=vocab_en['<pad>'], batch_first=True)\n",
    "    trg_batch = pad_sequence(trg_batch, padding_value=vocab_th['<pad>'], batch_first=True)\n",
    "\n",
    "    return src_batch.to(device), torch.tensor(src_len_batch, dtype=torch.int64).to(device), trg_batch.to(device)\n",
    "\n",
    "batch_size = 64  # Or your desired batch size\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_batch)\n",
    "\n",
    "# ✅ Performance Tracking\n",
    "performance_metrics = []\n",
    "\n",
    "def safe_exp(value):\n",
    "    return torch.exp(torch.tensor(value)).item() if value < 10 else float('inf')\n",
    "\n",
    "# ✅ Task 2: Implement Attention Mechanisms\n",
    "# ✅ Encoder\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, hid_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        self.rnn = nn.LSTM(emb_dim, hid_dim, bidirectional=True, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear = nn.Linear(2 * hid_dim, hid_dim)  # Linear layer for encoder outputs\n",
    "\n",
    "    def forward(self, src, src_len):\n",
    "        embedded = self.dropout(self.embedding(src))\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, src_len.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        packed_outputs, (hidden, cell) = self.rnn(packed_embedded)\n",
    "        outputs, _ = nn.utils.rnn.pad_packed_sequence(packed_outputs, batch_first=True)\n",
    "\n",
    "        hidden = (hidden[0] + hidden[1]) / 2\n",
    "        cell = (cell[0] + cell[1]) / 2\n",
    "\n",
    "        outputs = self.linear(outputs)  # Apply linear transformation to encoder outputs\n",
    "\n",
    "        return outputs, hidden, cell\n",
    "\n",
    "# ✅ Decoder\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout, attention):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        self.attention = attention\n",
    "        self.rnn = nn.LSTM(emb_dim + enc_hid_dim, dec_hid_dim, batch_first=True)\n",
    "        self.fc_out = nn.Linear(enc_hid_dim + dec_hid_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, input, hidden, cell, encoder_outputs):\n",
    "        input = input.unsqueeze(1)\n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "\n",
    "        attn_weights = self.attention(hidden, encoder_outputs).unsqueeze(1)\n",
    "        context = torch.bmm(attn_weights, encoder_outputs)\n",
    "\n",
    "        rnn_input = torch.cat((embedded, context), dim=2)\n",
    "        output, (hidden, cell) = self.rnn(rnn_input, (hidden.unsqueeze(0), cell.unsqueeze(0)))\n",
    "\n",
    "        prediction = self.fc_out(torch.cat((output.squeeze(1), context.squeeze(1)), dim=1))\n",
    "        return prediction, hidden.squeeze(0), cell.squeeze(0)\n",
    "\n",
    "# ✅ Seq2Seq Model\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, src, src_len, trg):\n",
    "        encoder_outputs, hidden, cell = self.encoder(src, src_len)\n",
    "        input_ = trg[:, 0]\n",
    "        outputs = []\n",
    "        for t in range(1, trg.shape[1]):\n",
    "            output, hidden, cell = self.decoder(input_, hidden, cell, encoder_outputs)\n",
    "            outputs.append(output)\n",
    "            input_ = output.argmax(1)\n",
    "        return torch.stack(outputs, dim=1)\n",
    "# ✅ General Attention (Dot Product)\n",
    "class GeneralAttention(nn.Module):\n",
    "    def __init__(self, enc_hid_dim, dec_hid_dim):  # Add __init__\n",
    "        super().__init__()\n",
    "        # You can add a linear layer here if needed for dimension matching in the future\n",
    "        # self.W = nn.Linear(dec_hid_dim, enc_hid_dim)  # Example if dimensions need adjustment\n",
    "\n",
    "    def forward(self, s, h):\n",
    "        # If you added a linear layer in __init__, use it here:\n",
    "        # s = self.W(s)\n",
    "        attn_scores = torch.bmm(h, s.unsqueeze(2)).squeeze(2)\n",
    "        return torch.softmax(attn_scores, dim=1)\n",
    "\n",
    "# ✅ Multiplicative Attention (Dot Product with Weight)\n",
    "class MultiplicativeAttention(nn.Module):\n",
    "    def __init__(self, enc_hid_dim, dec_hid_dim):  # Add __init__\n",
    "        super().__init__()\n",
    "        self.W = nn.Linear(dec_hid_dim, enc_hid_dim, bias=False)  # Corrected: dec_hid_dim -> enc_hid_dim\n",
    "\n",
    "    def forward(self, s, h):\n",
    "        s = self.W(s).unsqueeze(2)\n",
    "        attn_scores = torch.bmm(h, s).squeeze(2)\n",
    "        return torch.softmax(attn_scores, dim=1)\n",
    "\n",
    "# ✅ Additive Attention (Bahdanau)\n",
    "class AdditiveAttention(nn.Module):\n",
    "    def __init__(self, enc_hid_dim, dec_hid_dim):  # Add __init__\n",
    "        super().__init__()\n",
    "        self.W1 = nn.Linear(enc_hid_dim, dec_hid_dim)\n",
    "        self.W2 = nn.Linear(dec_hid_dim, dec_hid_dim)\n",
    "        self.v = nn.Linear(dec_hid_dim, 1, bias=False)\n",
    "\n",
    "    def forward(self, s, h):\n",
    "        s_expanded = self.W2(s).unsqueeze(1)\n",
    "        energy = torch.tanh(self.W1(h) + s_expanded)\n",
    "        attn_scores = self.v(energy).squeeze(2)\n",
    "        return torch.softmax(attn_scores, dim=1)\n",
    "\n",
    "\n",
    "# ✅ Task 3: Training and Evaluation for All Attention Types\n",
    "attention_types = {\"general\": GeneralAttention, \"multiplicative\": MultiplicativeAttention, \"additive\": AdditiveAttention}\n",
    "results = {}\n",
    "\n",
    "def plot_attention(attn_weights, input_tokens, output_tokens):\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    cax = ax.matshow(attn_weights.cpu().detach().numpy(), cmap='viridis')\n",
    "    fig.colorbar(cax)\n",
    "    ax.set_xticklabels([''] + input_tokens, rotation=90)\n",
    "    ax.set_yticklabels([''] + output_tokens)\n",
    "    plt.xlabel(\"Input Sequence\")\n",
    "    plt.ylabel(\"Output Sequence\")\n",
    "    plt.show()\n",
    "\n",
    "for attn_name, attn_class in attention_types.items():\n",
    "    print(f\"\\nTraining with {attn_name} attention...\")\n",
    "    \n",
    "    encoder = Encoder(len(vocab_en), 256, 512, 0.5).to(device)\n",
    "    attention = attn_class(512, 512).to(device)\n",
    "    decoder = Decoder(len(vocab_th), 256, 512, 512, 0.5, attention).to(device)\n",
    "    model = Seq2Seq(encoder, decoder).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.0005)\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=vocab_th['<pad>'])\n",
    "\n",
    "    training_losses, training_ppls, validation_losses, validation_ppls = [], [], [], []\n",
    "    start_time = time.time()\n",
    "\n",
    "    num_epochs = 5\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for src, src_len, trg in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            output = model(src, src_len, trg)\n",
    "            trg = trg[:, 1:].reshape(-1)\n",
    "            output = output.reshape(-1, output.shape[-1])\n",
    "            mask = trg != vocab_th['<pad>']\n",
    "            output, trg = output[mask], trg[mask]\n",
    "            loss = criterion(output, trg)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        train_loss /= len(dataloader)\n",
    "        training_losses.append(train_loss)\n",
    "        training_ppls.append(safe_exp(train_loss))\n",
    "        \n",
    "        model.eval()\n",
    "        valid_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for src, src_len, trg in dataloader:\n",
    "                output = model(src, src_len, trg)\n",
    "                trg = trg[:, 1:].reshape(-1)\n",
    "                output = output.reshape(-1, output.shape[-1])\n",
    "                mask = trg != vocab_th['<pad>']\n",
    "                output, trg = output[mask], trg[mask]\n",
    "                loss = criterion(output, trg)\n",
    "                valid_loss += loss.item()\n",
    "        valid_loss /= len(dataloader)\n",
    "        validation_losses.append(valid_loss)\n",
    "        validation_ppls.append(safe_exp(valid_loss))\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}: Train Loss: {train_loss:.4f}, Train PPL: {training_ppls[-1]:.4f}, Validation Loss: {valid_loss:.4f}, Validation PPL: {validation_ppls[-1]:.4f}\")\n",
    "    \n",
    "    end_time = time.time()\n",
    "    training_time = end_time - start_time\n",
    "    performance_metrics.append([attn_name, training_time])\n",
    "    \n",
    "    results[attn_name] = {\n",
    "        \"Training Loss\": training_losses,\n",
    "        \"Training PPL\": training_ppls,\n",
    "        \"Validation Loss\": validation_losses,\n",
    "        \"Validation PPL\": validation_ppls,\n",
    "    }\n",
    "\n",
    "def plot_losses(results):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    for attn_type, metrics in results.items():\n",
    "        plt.plot(metrics[\"Training Loss\"], label=f\"{attn_type} - Train Loss\")\n",
    "        plt.plot(metrics[\"Validation Loss\"], linestyle=\"dashed\", label=f\"{attn_type} - Validation Loss\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Training vs Validation Loss\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "plot_losses(results)\n",
    "\n",
    "# ✅ Print Performance Metrics\n",
    "print(\"\\nPerformance Metrics (Training Time per Attention Type):\")\n",
    "for attn_type, time_taken in performance_metrics:\n",
    "    print(f\"{attn_type}: {time_taken:.2f} seconds\")\n",
    "\n",
    "print(\"✅ Training Completed for All Attention Types\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "a3.venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
