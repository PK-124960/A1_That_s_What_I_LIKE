{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchtext, datasets, math\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1. Dataset Acquisition - Harry Potter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 57435\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 5897\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 6589\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"KaungHtetCho/Harry_Potter_LSTM\")\n",
    "\n",
    "print(dataset)\n",
    "\n",
    "# Load Dataset: The datasets library loads the Harry Potter dataset via load_dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(57435, 1)\n"
     ]
    }
   ],
   "source": [
    "print(dataset['train'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Task 2. Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Preprocessing the Text Data\n",
    "#### a) Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 57435/57435 [00:03<00:00, 16922.83 examples/s]\n",
      "Map: 100%|██████████| 5897/5897 [00:00<00:00, 13717.27 examples/s]\n",
      "Map: 100%|██████████| 6589/6589 [00:00<00:00, 13099.43 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tokens': ['harry', 'potter', 'and', 'the', 'sorcerer', \"'\", 's', 'stone']}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torchtext\n",
    "\n",
    "# The basic_english tokenizer breaks down the raw text into individual tokens.\n",
    "tokenizer = torchtext.data.utils.get_tokenizer('basic_english')\n",
    "\n",
    "\n",
    "# Tokenization is applied to all text examples in the dataset using:\n",
    "tokenize_data = lambda example: {'tokens': tokenizer(example['text'])}\n",
    "tokenized_dataset = dataset.map(tokenize_data, remove_columns=['text'])\n",
    "\n",
    "print(tokenized_dataset['train'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) Building the Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Vocabulary: Using torchtext.vocab.build_vocab_from_iterator, a vocabulary is created from tokenized data.\n",
    "# Words with a frequency less than 3 are excluded (min_freq=3).\n",
    "# Special tokens like <unk> (unknown) and <eos> (end of sequence) are added\n",
    "\n",
    "vocab = torchtext.vocab.build_vocab_from_iterator(tokenized_dataset['train']['tokens'], min_freq=3)\n",
    "vocab.insert_token('<unk>', 0)\n",
    "vocab.insert_token('<eos>', 1)\n",
    "vocab.set_default_index(vocab['<unk>'])  # Default token for unknown words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c) Preparing the Data for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numericalize Data: Each token is mapped to its corresponding index in the vocabulary, creating sequences of integers. \n",
    "# These sequences are reshaped into batches:\n",
    "\n",
    "def get_data(dataset, vocab, batch_size):\n",
    "    data = []\n",
    "    for example in dataset:\n",
    "        if example['tokens']:\n",
    "            tokens = example['tokens'] + ['<eos>']  # Add end-of-sequence token\n",
    "            tokens = [vocab[token] for token in tokens]\n",
    "            data.extend(tokens)\n",
    "    data = torch.LongTensor(data)\n",
    "    num_batches = data.shape[0] // batch_size\n",
    "    data = data[:num_batches * batch_size]\n",
    "    data = data.view(batch_size, num_batches)  # [batch size, seq len]\n",
    "    return data\n",
    "\n",
    "# Split Data: The dataset is divided into train, validation, and test sets for model evaluation.\n",
    "batch_size = 128\n",
    "train_data = get_data(tokenized_dataset['train'], vocab, batch_size)\n",
    "valid_data = get_data(tokenized_dataset['validation'], vocab, batch_size)\n",
    "test_data  = get_data(tokenized_dataset['test'], vocab, batch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Model Architecture and Training Process\n",
    "#### a) Model Architecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Architecture: The language model is an LSTM-based recurrent neural network with the following key components:\n",
    "\n",
    "# Embedding Layer:\n",
    "    # Maps each token (integer) to a dense vector of size emb_dim.\n",
    "    # Provides semantic meaning for tokens.\n",
    "# LSTM Layers:\n",
    "    # A stack of 2 LSTM layers processes the input sequence (num_layers=2).\n",
    "    # Each LSTM cell has a hidden state dimension of hid_dim=1024.\n",
    "    # Dropout (dropout_rate=0.65) reduces overfitting.\n",
    "# Fully Connected Layer:\n",
    "    # Maps the output of the LSTM layers to the vocabulary size.\n",
    "    # Predicts the next token in the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class LSTMLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim, hid_dim, num_layers, dropout_rate):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hid_dim = hid_dim\n",
    "        self.emb_dim = emb_dim\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_dim)\n",
    "        self.lstm = nn.LSTM(emb_dim, hid_dim, num_layers=num_layers, dropout=dropout_rate, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.fc = nn.Linear(hid_dim, vocab_size)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "# Initialization: All weights are initialized uniformly to stabilize training\n",
    "    def init_weights(self):\n",
    "        init_range_emb = 0.1\n",
    "        init_range_other = 1 / math.sqrt(self.hid_dim)\n",
    "        self.embedding.weight.data.uniform_(-init_range_emb, init_range_other)\n",
    "        self.fc.weight.data.uniform_(-init_range_other, init_range_other)\n",
    "        self.fc.bias.data.zero_()\n",
    "        for i in range(self.num_layers):\n",
    "            self.lstm.all_weights[i][0] = torch.FloatTensor(self.emb_dim, self.hid_dim).uniform_(-init_range_other, init_range_other)\n",
    "            self.lstm.all_weights[i][1] = torch.FloatTensor(self.hid_dim, self.hid_dim).uniform_(-init_range_other, init_range_other)\n",
    "\n",
    "    def init_hidden(self, batch_size, device):\n",
    "        hidden = torch.zeros(self.num_layers, batch_size, self.hid_dim).to(device)\n",
    "        cell = torch.zeros(self.num_layers, batch_size, self.hid_dim).to(device)\n",
    "        return hidden, cell\n",
    "\n",
    "    def detach_hidden(self, hidden):\n",
    "        hidden, cell = hidden\n",
    "        hidden = hidden.detach()\n",
    "        cell = cell.detach()\n",
    "        return hidden, cell\n",
    "\n",
    "    def forward(self, src, hidden):\n",
    "        embedding = self.dropout(self.embedding(src))  # [batch-size, seq len, emb dim]\n",
    "        output, hidden = self.lstm(embedding, hidden)  # [batch size, seq len, hid dim]\n",
    "        output = self.dropout(output)\n",
    "        prediction = self.fc(output)  # [batch_size, seq_len, vocab_size]\n",
    "        return prediction, hidden\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) Training Process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Loop: For each batch, the input sequence (src) and target sequence (target) are generated using:\n",
    "\n",
    "def get_batch(data, seq_len, idx):\n",
    "    #data #[batch size, bunch of tokens]\n",
    "    src    = data[:, idx:idx+seq_len]                   \n",
    "    target = data[:, idx+1:idx+seq_len+1]  #target simply is ahead of src by 1            \n",
    "    return src, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data, optimizer, criterion, batch_size, seq_len, clip, device):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    model.train()\n",
    "    # drop all batches that are not a multiple of seq_len\n",
    "    # data #[batch size, seq len]\n",
    "    num_batches = data.shape[-1]\n",
    "    data = data[:, :num_batches - (num_batches -1) % seq_len]  #we need to -1 because we start at 0\n",
    "    num_batches = data.shape[-1]\n",
    "    \n",
    "    #reset the hidden every epoch\n",
    "    hidden = model.init_hidden(batch_size, device)\n",
    "    \n",
    "    for idx in tqdm(range(0, num_batches - 1, seq_len), desc='Training: ',leave=False):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        #hidden does not need to be in the computational graph for efficiency\n",
    "        hidden = model.detach_hidden(hidden)\n",
    "\n",
    "        src, target = get_batch(data, seq_len, idx) #src, target: [batch size, seq len]\n",
    "        src, target = src.to(device), target.to(device)\n",
    "        batch_size = src.shape[0]\n",
    "        # The model predicts the next token using:\n",
    "        prediction, hidden = model(src, hidden)               \n",
    "\n",
    "        #need to reshape because criterion expects pred to be 2d and target to be 1d\n",
    "        prediction = prediction.reshape(batch_size * seq_len, -1)  #prediction: [batch size * seq len, vocab size]  \n",
    "        target = target.reshape(-1)\n",
    "        # The loss is computed using CrossEntropyLoss\n",
    "        loss = criterion(prediction, target)\n",
    "        \n",
    "        # Gradients are backpropagated, and weights are updated using the Adam optimizer:\n",
    "        loss.backward()\n",
    "        # Gradient clipping (clip=0.25) ensures stable training:\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item() * seq_len\n",
    "    return epoch_loss / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data, criterion, batch_size, seq_len, device):\n",
    "\n",
    "    epoch_loss = 0\n",
    "    model.eval()\n",
    "    num_batches = data.shape[-1]\n",
    "    data = data[:, :num_batches - (num_batches -1) % seq_len]\n",
    "    num_batches = data.shape[-1]\n",
    "\n",
    "    hidden = model.init_hidden(batch_size, device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx in range(0, num_batches - 1, seq_len):\n",
    "            hidden = model.detach_hidden(hidden)\n",
    "            src, target = get_batch(data, seq_len, idx)\n",
    "            src, target = src.to(device), target.to(device)\n",
    "            batch_size= src.shape[0]\n",
    "\n",
    "            prediction, hidden = model(src, hidden)\n",
    "            prediction = prediction.reshape(batch_size * seq_len, -1)\n",
    "            target = target.reshape(-1)\n",
    "\n",
    "            loss = criterion(prediction, target)\n",
    "            epoch_loss += loss.item() * seq_len\n",
    "    return epoch_loss / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:\n",
      "\tTrain Perplexity: 516.267\n",
      "\tValid Perplexity: 312.083\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2:\n",
      "\tTrain Perplexity: 247.631\n",
      "\tValid Perplexity: 152.466\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3:\n",
      "\tTrain Perplexity: 152.147\n",
      "\tValid Perplexity: 113.978\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4:\n",
      "\tTrain Perplexity: 120.241\n",
      "\tValid Perplexity: 98.783\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5:\n",
      "\tTrain Perplexity: 103.616\n",
      "\tValid Perplexity: 90.179\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6:\n",
      "\tTrain Perplexity: 92.929\n",
      "\tValid Perplexity: 84.597\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7:\n",
      "\tTrain Perplexity: 84.814\n",
      "\tValid Perplexity: 80.463\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8:\n",
      "\tTrain Perplexity: 78.236\n",
      "\tValid Perplexity: 77.509\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9:\n",
      "\tTrain Perplexity: 73.009\n",
      "\tValid Perplexity: 75.296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10:\n",
      "\tTrain Perplexity: 68.483\n",
      "\tValid Perplexity: 73.751\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11:\n",
      "\tTrain Perplexity: 64.871\n",
      "\tValid Perplexity: 72.207\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12:\n",
      "\tTrain Perplexity: 61.609\n",
      "\tValid Perplexity: 70.873\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13:\n",
      "\tTrain Perplexity: 58.663\n",
      "\tValid Perplexity: 70.029\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14:\n",
      "\tTrain Perplexity: 56.337\n",
      "\tValid Perplexity: 69.555\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15:\n",
      "\tTrain Perplexity: 54.002\n",
      "\tValid Perplexity: 68.965\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16:\n",
      "\tTrain Perplexity: 51.981\n",
      "\tValid Perplexity: 68.296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17:\n",
      "\tTrain Perplexity: 50.236\n",
      "\tValid Perplexity: 68.405\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18:\n",
      "\tTrain Perplexity: 48.575\n",
      "\tValid Perplexity: 67.821\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19:\n",
      "\tTrain Perplexity: 47.057\n",
      "\tValid Perplexity: 67.467\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20:\n",
      "\tTrain Perplexity: 45.619\n",
      "\tValid Perplexity: 67.472\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21:\n",
      "\tTrain Perplexity: 44.257\n",
      "\tValid Perplexity: 67.253\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22:\n",
      "\tTrain Perplexity: 42.947\n",
      "\tValid Perplexity: 67.177\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23:\n",
      "\tTrain Perplexity: 41.883\n",
      "\tValid Perplexity: 66.773\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24:\n",
      "\tTrain Perplexity: 40.779\n",
      "\tValid Perplexity: 66.995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25:\n",
      "\tTrain Perplexity: 39.789\n",
      "\tValid Perplexity: 67.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26:\n",
      "\tTrain Perplexity: 38.783\n",
      "\tValid Perplexity: 66.821\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27:\n",
      "\tTrain Perplexity: 37.921\n",
      "\tValid Perplexity: 67.074\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28:\n",
      "\tTrain Perplexity: 37.106\n",
      "\tValid Perplexity: 67.289\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29:\n",
      "\tTrain Perplexity: 36.248\n",
      "\tValid Perplexity: 67.871\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30:\n",
      "\tTrain Perplexity: 35.565\n",
      "\tValid Perplexity: 68.523\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31:\n",
      "\tTrain Perplexity: 34.879\n",
      "\tValid Perplexity: 68.372\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32:\n",
      "\tTrain Perplexity: 34.235\n",
      "\tValid Perplexity: 68.812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33:\n",
      "\tTrain Perplexity: 33.652\n",
      "\tValid Perplexity: 68.290\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34:\n",
      "\tTrain Perplexity: 32.945\n",
      "\tValid Perplexity: 69.306\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35:\n",
      "\tTrain Perplexity: 32.414\n",
      "\tValid Perplexity: 69.373\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36:\n",
      "\tTrain Perplexity: 31.867\n",
      "\tValid Perplexity: 69.967\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37:\n",
      "\tTrain Perplexity: 31.379\n",
      "\tValid Perplexity: 69.882\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38:\n",
      "\tTrain Perplexity: 30.743\n",
      "\tValid Perplexity: 70.405\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39:\n",
      "\tTrain Perplexity: 30.341\n",
      "\tValid Perplexity: 70.952\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40:\n",
      "\tTrain Perplexity: 29.889\n",
      "\tValid Perplexity: 70.735\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41:\n",
      "\tTrain Perplexity: 29.487\n",
      "\tValid Perplexity: 71.093\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42:\n",
      "\tTrain Perplexity: 29.069\n",
      "\tValid Perplexity: 71.712\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43:\n",
      "\tTrain Perplexity: 28.747\n",
      "\tValid Perplexity: 71.507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44:\n",
      "\tTrain Perplexity: 28.284\n",
      "\tValid Perplexity: 71.744\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45:\n",
      "\tTrain Perplexity: 27.972\n",
      "\tValid Perplexity: 72.116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46:\n",
      "\tTrain Perplexity: 27.620\n",
      "\tValid Perplexity: 71.767\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47:\n",
      "\tTrain Perplexity: 27.291\n",
      "\tValid Perplexity: 72.441\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48:\n",
      "\tTrain Perplexity: 26.946\n",
      "\tValid Perplexity: 72.581\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49:\n",
      "\tTrain Perplexity: 26.712\n",
      "\tValid Perplexity: 73.117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50:\n",
      "\tTrain Perplexity: 26.325\n",
      "\tValid Perplexity: 73.195\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Initialize model\n",
    "model = LSTMLanguageModel(len(vocab), emb_dim=1024, hid_dim=1024, num_layers=2, dropout_rate=0.65).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training and evaluation loop\n",
    "n_epochs = 50\n",
    "seq_len = 50  # Decoding length\n",
    "clip = 0.25\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    train_loss = train(model, train_data, optimizer, criterion, batch_size, seq_len, clip, device)\n",
    "    valid_loss = evaluate(model, valid_data, criterion, batch_size, seq_len, device)\n",
    "\n",
    "    print(f'Epoch {epoch+1}:')\n",
    "#     Validation:\n",
    "        # After each epoch, the model is evaluated on the validation set to monitor performance.\n",
    "        # Perplexity, a measure of language model quality, is reported:\n",
    "    print(f'\\tTrain Perplexity: {math.exp(train_loss):.3f}')\n",
    "    print(f'\\tValid Perplexity: {math.exp(valid_loss):.3f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'best-val-lstm_lm.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Perplexity: 96.453\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('best-val-lstm_lm.pt',  map_location=device))\n",
    "test_loss = evaluate(model, test_data, criterion, batch_size, seq_len, device)\n",
    "print(f'Test Perplexity: {math.exp(test_loss):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Real-world inference\n",
    "\n",
    "Here we take the prompt, tokenize, encode and feed it into the model to get the predictions.  We then apply softmax while specifying that we want the output due to the last word in the sequence which represents the prediction for the next word.  We divide the logits by a temperature value to alter the model’s confidence by adjusting the softmax probability distribution.\n",
    "\n",
    "Once we have the Softmax distribution, we randomly sample it to make our prediction on the next word. If we get <unk> then we give that another try.  Once we get <eos> we stop predicting.\n",
    "    \n",
    "We decode the prediction back to strings last lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(prompt, max_seq_len, temperature, model, tokenizer, vocab, device, seed=None):\n",
    "    if seed is not None:\n",
    "        torch.manual_seed(seed)\n",
    "    model.eval()\n",
    "    tokens = tokenizer(prompt)\n",
    "    indices = [vocab[t] for t in tokens]\n",
    "    batch_size = 1\n",
    "    hidden = model.init_hidden(batch_size, device)\n",
    "    with torch.no_grad():\n",
    "        for i in range(max_seq_len):\n",
    "            src = torch.LongTensor([indices]).to(device)\n",
    "            prediction, hidden = model(src, hidden)\n",
    "            \n",
    "            #prediction: [batch size, seq len, vocab size]\n",
    "            #prediction[:, -1]: [batch size, vocab size] #probability of last vocab\n",
    "            \n",
    "            probs = torch.softmax(prediction[:, -1] / temperature, dim=-1)  \n",
    "            prediction = torch.multinomial(probs, num_samples=1).item()    \n",
    "            \n",
    "            while prediction == vocab['<unk>']: #if it is unk, we sample again\n",
    "                prediction = torch.multinomial(probs, num_samples=1).item()\n",
    "\n",
    "            if prediction == vocab['<eos>']:    #if it is eos, we stop\n",
    "                break\n",
    "\n",
    "            indices.append(prediction) #autoregressive, thus output becomes input\n",
    "\n",
    "    itos = vocab.get_itos()\n",
    "    tokens = [itos[i] for i in indices]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "harry potter is a bad man , i reckon you ' d better go back to bed , and i ' m going to go to the hospital wing .\n",
      "\n",
      "0.7\n",
      "harry potter is a bad man , i reckon you ' d go to bed .\n",
      "\n",
      "0.75\n",
      "harry potter is . he didn ' t need him , but he didn ' t like a single idea of a very good time quidditch , it wasn ' t a attack\n",
      "\n",
      "0.8\n",
      "harry potter is . he didn ' t need him , but he didn ' t like a single family .\n",
      "\n",
      "1.0\n",
      "harry potter is . he didn ' t need him , but he didn ' t like good for his hand to eat the seeing .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = 'Harry Potter is '\n",
    "max_seq_len = 30\n",
    "seed = 0\n",
    "\n",
    "#smaller the temperature, more diverse tokens but comes \n",
    "#with a tradeoff of less-make-sense sentence\n",
    "temperatures = [0.5, 0.7, 0.75, 0.8, 1.0]\n",
    "for temperature in temperatures:\n",
    "    generation = generate(prompt, max_seq_len, temperature, model, tokenizer, \n",
    "                          vocab, device, seed)\n",
    "    print(str(temperature)+'\\n'+' '.join(generation)+'\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
